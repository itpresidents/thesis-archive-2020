{"post_id":543,"student_id":"15","student_name":"Michael Kripchak","student_slug":"michael-kripchak","advisor_id":"114","advisor_name":"Adaora Udoji","advisor_slug":"aeu4","project_title":"Touch at a Distance","project_question":"This research addresses how to enhance telepresence with the use of haptic feedback in a way that generates, for users, the perception of a more 'real,' face-to-face experience. The solution consists of the creation of a real-time virtual reality (VR) experience that allows two users to share a virtual space as holograms. Telepresence is generated by employing body and hand tracking 3D cameras to project a person’s real-time image into a VR environment. Both users wear glove-like devices that place vibration motors on their fingertips. This allows them to reach out and not only touch the other person’s holographic representation, but to <i>feel</i> the interaction through the haptic response.","short_description":"Have you ever wanted to hug someone on the other side of the planet? Touch is one of the most powerful senses we have. It has also become increasingly absent as our interpersonal communications move online. This project is a first step in adding haptics to our virtual representations of self.","further_reading":"<p>My journey to this thesis begins during a conversation with a friend about using VR to re-physicalize our media. He went for a 15 minute walk and came back with the idea of cohabiting that experience with someone else. In that instant, the idea became about re-physicalizing our connection to each other. Have you ever wanted to hug someone on the other side of the planet? I know soldiers have. I know traveling moms and dads have. My thesis is about creating 'tangible telepresence': Adding a sense of touch to the virtual projection of someone who is in another location.</p>\n<p><b>Motivations</b></p>\n<p>The medium I’m using for this project is VR—specifically, an HTC Vive. When most people think of VR, they imagine having fantastical experiences or visiting distant lands—both of which are found in great volume on the medium. Another truth about VR, though, is that it often leaves us disconnected from each other—strapped into a headset, flailing around at the air. I believe we have been missing the point of what VR can bring to society.</p>\n<p>To me, the most impactful inventions are those that have shrunk the planet. The airplane. The telephone. The internet. These have all brought us closer together, and made it easier for us to connect with each other. These new connections, in turn, have profoundly changed the everyday lives of people. I believe VR, with the proper enhancements to its current capabilities, has the potential to truly become one of these technologies. Its ability to create a sense of presence—of being there—can be used to create a sense of being with someone who is actually in another location.</p>\n<p>We’ve already seen the first steps toward this with experiences like Facebook’s VR Spaces and virtual communities like VR Chat. People are also using VR to maintain long distance relationships. It’s even predicted that due to budgetary concerns, in the near future, many of our face-to-face business exchanges and transactions will occur virtually. While the visual and auditory experience has been the focus of development in these areas, the critical sense of touch has been sorely absent.</p>\n<p><b>Building the Experience</b></p>\n<p><b>Body Sensing</b><br />\nI used two devices to translate people’s bodies into VR: Microsoft’s Kinect 2 and the Leap Motion. For gross body sensing, I used the Kinect. Kinect is a motion sensing device that consists of a visual spectrum camera and an infrared camera that captures depth data of the scene it is viewing. With this information, it is able to plot a subject’s virtual joints in 3D space. For fine-grained hand tracking, I used the Leap Motion. This device also employs an infrared camera to create a virtual representation of joints in 3D, but it is designed specifically to capture hands.</p>\n<p><b>Creating a Sense of Touch</b><br />\nFor the haptic portion of the project, I focused on creating a sense of touch for the hands. I reached out to several companies that are working on haptic solutions for VR, but the timeline for this thesis project did not permit collaboration. Therefore, I decided to use a prototype I created in 2016 for Ms. Benadetta Piantella’s Physical Computing class, consisting of small vibration motors and an Arduino Uno board. </p>\n<p><b>Building the Haptic Device</b><br />\nFor PComp, I originally wanted to design a new way to simulate pressure against your skin. I experimented with electromagnets to create a hand-worn device that would have tiny “cells” that could do just that. The project didn’t succeed. I wasn’t able to produce a strong enough force due to the amount of current I could produce and also because of the heat that was generated in the electromagnet. I also considered using pneumatics or hydraulics, but read that the rate of pressure change would be too slow for my purposes. For these reasons, I chose to stay with the most popular method among VR glove makers: Vibration motors. For the prototype, I used an Arduino Uno and five vibration motors, one placed on each of the fingertips. Our fingertips contain some of the highest concentrations of cutaneous sensory neurons in the body, making them extremely sensitive to tactile stimulation, or touch.</p>\n<p><b>Virtual Projection</b><br />\nFor the software-side of the experience, I used Unreal Engine 4 (a middleware software used to create 3D environments and real-time interactions), the Kinect for Unreal plugin created by Opaque Multimedia, the UE4duino Arduino plug-in by Rodrigo Villani, and the UE4 Leap Motion plugin by getnamo. For projecting the Kinect data into 3D virtual space, I received a tremendous amount of help from Mr. Todd Bryant. He modified Blueprints (code) that were originally intended to display prerecorded volumetric video from DepthKit to display the real-time Kinect data. </p>\n<p><b>Interaction</b><br />\nWith the volumetric video inside my Unreal project, I began working on the interaction. Since I was unable to use the video feed to test for collisions, I used the joint data from the Kinect and mapped the joints to the video frame coordinates. I then attached non-visual collision objects to the joints of the body in the video feed. With these collision boxes, I could then check for when the user’s hands - captured via the Leap Motion plug-in - were in the same location as the body on the video feed. This collision would send a signal to the Arduino Uno.</p>\n<p><b>Setup</b><br />\nThe final setup consisted of a double of everything: two Kinects, two PCs, two Vive setups, and two haptic devices. One person’s experience is fed data from the Kinect pointed at the other person, and vice versa. For this reason, the VR areas need to be physically close to each other so that the Kinects’ cables can reach the opposite computers.</p>\n<p><b>Playtesting</b><br />\nTest users had very positive responses to the experience. All of them thought that the haptic sensation added to the interaction with the hologram, and missed having it when it was removed from the experience. One tester felt that it made a bigger difference than she was expecting. She also had the great suggestion of having the vibrations gradually increase instead of abruptly turning on and off.</p>\n<p><b>The Future</b><br />\nThe most obvious area to further the project is through a greater surface area of haptic feedback. There are some full-body haptic suits currently in development, such as the Teslasuit. This would expand the experience beyond just hands. I’d also like to increase the fidelity of the haptics, perhaps with HaptX’s new VR glove that uses microfluidics, or Contact CI’s Maestro glove as it actually restricts finger movement. </p>\n<p>I’d also like to translate the project into a web browser-based WebVR experience. I was originally working in THREE.js and using Kinectron to bring in the person’s holographic representation. I had the Kinect data in VR and was working on collision detection when I migrated to the Unreal Engine due to my greater familiarity with it. A browser-based experience would eliminate the need for a specialized app and could allow for a truly remote telepresence experience.</p>\n<p>Test users referred to the haptic response in the experience as a reward. The interesting thing is that basic human contact actually interacts with our pleasure sensors - making being touched a reward in and of itself. Maybe this artificial experience is causing a similar physiological response to real human touch.</p>\n<p>There’s a phenomenon in neuroscience and psychology called brain-to-brain coupling. This happens at the times when we feel most connected to those around us, either through empathy or interaction, and is enhanced by physical contact. Physiologically, during periods of brain-to-brain coupling, our brain waves actually become synchronized. This correlates to enhanced learning and can even lead to pain reduction (PNAS, 2018). If tangible telepresence causes similar physiological effects to real touch, then perhaps brain-to-brain coupling can be induced, synchronizing our brain waves over large distances. If this is possible, then we may have a new form of communication that can truly shrink the planet and bring us closer together.</p>\n<p><i>Goldstein,Pavel, et al. \"Brain-to-brain coupling during handholding is associated with pain reduction” Proceedings of the National Academy of Sciences of the United States of America, February 26, 2018. www.pnas.org.</i></p>\n","portfolio_icon":{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/KripchakThesisThumbnail.png","title":"Kripchak Thesis Thumbnail","alt":"Telepresence projection in VR.","caption":"Screen-capture of holographic telepresence from within VR."},"topics":[{"name":"Tangiable","slug":"tangiable"},{"name":"VR\\AR","slug":"vr-ar"}],"video_presentation_url":"https://player.vimeo.com/video/270206402","video_documentation_url":"","project_url":"","description":"","featured_image":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/KripchakThesisThumbnail.png","title":"Kripchak Thesis Thumbnail","alt":"Telepresence projection in VR.","caption":"Screen-capture of holographic telepresence from within VR."}],"slide_show":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/ThesisGifObjects04.gif","title":"Collision Object Tracking","alt":"Volumetric video and 3d objects tracking user's head.","caption":"Testing collision object head tracking and scaling with Kinect volumetric video."},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/ThesisGifLeap02.gif","title":"Leap Motion Tests","alt":"Leap Motion hand mesh and VR projection.","caption":"Testing the Leap Motion hand tracking and the Kinect projection in VR."},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/ThesisPic03.png","title":"The Haptic Device","alt":"Arduino haptic device attached to hand","caption":"Ardunio-powered haptic device, connected to vibration motors at fingertips."},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/ThesisPic04.png","title":"User Testing","alt":"Test user with arm raised.","caption":"Grau Recarens user testing the tangible telepresence experience."}]}