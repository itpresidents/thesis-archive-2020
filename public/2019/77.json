{"student_id":"77","student_name":"Krizia Fernando","student_slug":"krizia-fernando","advisor_name":"Stefani Bardin","title":"Emojispeak: Accessible Emojis","thesis_statement":"EmojiSpeak is a platform that attempts to expose how emoji is currently interpreted on screenreaders, through a text-to-speech message board simulator; and a community-driven emoji dictionary to mitigate its visual inequity on the web. The goal of a crowdsourced dictionary is to encourage culturemaking between sighted allies and blind users.","abstract":"The rise of “visual content as interface” on touch screens and mobile devices has paved the way for users to adopt emojis in daily conversations since 1999 when Shigetaka Kurita first released the original 176 emoji for mobile phones, through Japanese telecom, NTT DOCOMO.<br><br>\r\n\r\nIn recent years, emoji has evolved in usage as it started appearing in court documents, and in 2015, the “Face with Tears of Joy” emoji landed as the Oxford Dictionary Word of the Year.\r\n<br><br>\r\nAs online interactions become increasingly visual and less accessible to people with vision impairments, I’m proposing to redesign emoji through a platform that enables a community to redefine emoji’s context, its usage and meaning, through a crowdsourced dictionary and a message board simulator with Text-to-Speech.<br><br>\r\n \r\nMy project aims to inquire about the visual inequity of emoji’s. Who is excluded from this social phenomenon? How do emojis appear in screenreaders? Microsoft’s Inclusive Design Toolkit encourages that designers find touchpoints of exclusion in their design process. Their principle is heavily inspired by the Social Model of Disability, which states that “Removing barriers creates equality and offers disabled people more independence, choice and control.” The Social Model of Disability is a new way of approach that contradicts medical model of disability which focuses on people as being disabled by their impairments or differences. The medical model looks at what is 'wrong' with the person, not what the person needs. This creates low expectations and leads to people losing independence, choice and control in their lives.","context_research":"The objective of the research is to explore emoji through the lens of web accessibility, and conduct a “solve for one, extend to many” inclusive design principle ultilized by Microsoft. The research is based on a case study with a completely blind user, who uses three applications to tweet a message with emoji.\r\n<br><br>\r\nDo emoji present visual inequity in communication? Frequent exchange of visual media like memes, gifs or emoji contribute to “Glance Culture”, a term coined by Damon Rose, a blind BBC reporter. Rose exposes the barrier that emoji presents in communication, in which he stated that emoji adds to an exclusionary experience for the blind community as it is used frequently with not much reference.<br><br>\r\n\r\nFor this particular study, the researcher will be focusing on visually impaired users. CDC reports that there are twenty-one million Americans report functional vision problems or eye conditions that may compromise vision.\r\n<br><br>\r\nBased on key findings that came out of the initial user research, there are two main challenges that contribute to the visual inequity of emoji: Firstly, existing technical barriers, or how descriptions are read through screenreaders. This is a texting etiquette issue for blind users which excludes them from the culture of using emoji. Secondly, existing language barrier of emoji becoming a substitutive and highly visual language. Emoji leans toward oculo-centricism/visual first bias of graphical user interfaces, to compensate for the inability to convey facial expression, tone of voice and gesture in digital communication.","technical_details":"To populate the emoji dictionary, EmojiSpeak uses Twemoji, Twitter’s open-source emoji, with the data dictionary derived from an existing library https://github.com/iamcal/emoji-data. Text-to-speech will be added as a feature on the message board, using React.JS.<br><br>\r\n\r\nThe platform with be a web application that uses HTML, Bootstrap and Javascript.","further_reading":"<p>Project Statement</p>\n<p>\"Twitter and other social media platforms have become increasingly visual, over the past decade as media such as photos, videos, and GIFs have become more prevalent as content.\" [1]. From text conversations, e-mail subject lines, to social media platforms, visual media has taken over modern day channels of conversations, as online interactions become increasingly visual and less accessible to people with vision impairments and screen reader users. I’m exploring emojis by designing a way to add alternative image descriptions, and exposing how emoji sound on the screen.</p>\n<p>Motivation</p>\n<p>The “face with tears of joy” emoji became the Oxford English Dictionary word of the year in 2015, [1] and this event signals how emojis have become part of a social phenomenon and in shaping language. Many research on emojis are leaning towards sentiment analysis and the use of machine learning [4] to sift through large amount of data on Twitter and other social media platforms, but the question of emojis as accessible visual media has not been explored in-depth. Just recently new emoji designs were added to iOS to represent different types of people with disabilities.</p>\n<p>Social Model of Disability.</p>\n<p>My project aims to inquire about the visual inequity of emoji’s. How are people being excluded from this social phenomenon? How do emojis appear in screenreaders?  Social Model of Disability is a new way of approach that contradicts medical model of disability which focuses on people as being disabled by their impairments or differences. The medical model looks at what is 'wrong' with the person, not what the person needs. This creates low expectations and leads to people losing independence, choice and control in their lives. [5] Microsoft’s Inclusive Design Toolkit encourages that designers find touch points of exclusion. This is heavily inspired by the Social Model of Disability, which states that “Removing these barriers creates equality and offers disabled people more independence, choice and control.” Example given on scope.org: Your child with a visual impairment wants to read the latest best-selling book, so they can chat about it with their friends. The social model solution makes full-text recordings available when the book is published.</p>\n<p>Emojis becoming a substitutive language in communication. [1] They are so pervasive that it has become a part of court documents: Emoji are<a href=\"https://www.theverge.com/2019/2/18/18225231/emoji-emoticon-court-case-reference\"> showing up as evidence in court more frequently with each passing year</a>. Between 2004 and 2019, there was an exponential rise in emoji and emoticon references in US court opinions, with over 30 percent of all cases appearing in 2018, according to Santa Clara University law professor Eric Goldman, who has been tracking all of the references to \"emoji\" and \"emoticon\" that show up in US court opinions. So far, the emoji and emoticons have rarely been important enough to sway the direction of a case, but as they become more common, the ambiguity in how emoji are displayed and what we interpret emoji to mean could become a larger issue for courts to contend with.[3]</p>\n<p>An exploration of improving alt-text in micro-interactions. Many stakeholders on the side of creating digital experiences are just starting to understand the impact of adding image descriptions (technically called \"alt text\"), but not alot of people are motivated to do this in practice. Instagram recently did this update on social media, where users can add descriptions on their photos. (An anonymous accessibility expert who follows my<a href=\"http://instagram.com/100DaysofInclusiveDesign\"> @100DaysofInclusiveDesign</a> posts).</p>\n<p>Delightful social interaction as design criteria in digital accessibility. In my research, I’ve noticed that exploring social criteria in digital accessibility and or an individual’s self-expression is not widely considered. In the context of this study, the fun experience of using emojis in text conversations does not translate similarly in speech interface.</p>\n<p>Competitive Analysis</p>\n<p>Veronica Lewis, a blogger who has low vision, explains on her tech review “<a href=\"http://www.perkinselearning.org/technology/blog/how-do-people-vision-impairments-use-emoji\">How Visually Impaired How Do People with Vision Impairments Use Emoji?</a>” [6]</p>\n<p><i>“People use emoji's to convey different messages. As someone with a vision impairment, it can be hard to keep up with understanding what they are and how they are used. One of the users she interviews directly states, “when I am confused with emojis in text, I go to emojipedia to understand their meaning.” This article also explains the problem with texting etiquette in emoji where “typing several in a row can be annoying especially when it is translated from text to speech”</i></p>\n<p>Emojipedia is the biggest resource for emoji and their origin. Although many users use emojipedia as emoji reference, it is a closed system, authored by researchers and writers. I am looking for a community-driven solution that also is a way to build empathy around the issue of accessibility and image descriptions.</p>\n<p>Urban Dictionary invites anyone to add a word and define it, just like how regular dictionaries work. It would be the closest product solution I have in mind, in terms of creating a community-driven platform and contextualizing a visual language like emoji. The content being very text oriented helps with compatibility to screen readers and text-to-speech technology.</p>\n<p>Rapid Prototyping</p>\n<p>As an intervention, I have prototyped possible solutions to present an alternative way to access emoji by testing two solutions. Solution A is a crowdsourced dictionary that opens up each emoji for different interested users (blind or sighted) to submit their own take on the context of use of the particular emoji. Solution B is an message board interface that speaks out the current image description of the emoji to invite users to understand how emoji is currently labeled as provided by the Unicode Consortium.</p>\n<p>Solution A: Emoji Dictionary</p>\n<p>To get a community platform to provide context of what an emoji means, I created a prototype that invites users to have</p>\n<ul>\n<li>A way to contribute and submit their usage context to emoji.</li>\n<li>text alternatives generated by Unicode</li>\n</ul>\n<p><a href=\"https://www.instagram.com/p/BwIbNQglS_I/?utm_source=ig_web_copy_link\">View Demo Here</a></p>\n<p>Solution B: Emoji Composer</p>\n<p>To expose the original alternative text of emoji, I have created a message board that uses Text-to-Speech technology</p>\n<ul>\n<li>Listening to message or a tweet - a button that lets users listen in to the emoji’s image description or technically what is called “alternate text”, is a required accessibility feature that speaks out an image description</li>\n<li>Offline/online toggle - adds a safety net for the user for error-proof message composition</li>\n</ul>\n<p><a href=\"https://www.instagram.com/p/BvzSsRUlplg/?utm_source=ig_web_copy_link\">View Demo Here</a></p>\n<p>Prototype Feedback:</p>\n<p><i>“How the emoji is presenting itself on the web seems like the first problem that needs to be solved. Fix that and you have already fixed its usability.”</i></p>\n<p><i>“How is this solving glance culture? Maybe reimagine how you can redesign of the emoji keyboard? How can one access it without a screen? Categorization? How is the gesture like”</i></p>\n<p><i>“If you are in design space, and have enough time, what can be don to create a multimodal approach?” Like a heart emoji, sounds like a beating heart, and provides a pattern of haptic feedback?</i></p>\n<p>References:</p>\n<p>[1] Semiotics of Emojis<br />\n<a href=\"https://www.cogitatiopress.com/mediaandcommunication/article/viewFile/1041/1041\">https://www.cogitatiopress.com/mediaandcommunication/article/viewFile/1041/1041</a></p>\n<p>[2] Twitter Image Descriptions<br />\n<a href=\"https://www.colegleason.com/static/papers/TwitterImageDescriptions_WebConf2019.pdf\">https://www.colegleason.com/static/papers/TwitterImageDescriptions_WebConf2019.pdf</a></p>\n<p>[3] Emojis in Court Documents<br />\n<a href=\"https://yro.slashdot.org/story/19/02/19/0836238/emoji-are-showing-up-in-court-cases-exponentially-and-courts-arent-prepared?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed\">https://yro.slashdot.org/story/19/02/19/0836238/emoji-are-showing-up-in-court-cases-exponentially-and-courts-arent-prepared?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</a></p>\n<p>[4] Instagram’s analysis of emojis<br />\n<a href=\"https://instagram-engineering.com/tagged/emoji\">https://instagram-engineering.com/tagged/emoji</a></p>\n<p>[5] Social Model of Disability<br />\n<a href=\"https://www.scope.org.uk/about-us/social-model-of-disability\">https://www.scope.org.uk/about-us/social-model-of-disability</a></p>\n<p>[6] Emoji’s and Screenreaders<br />\n<a href=\"http://www.perkinselearning.org/technology/blog/how-do-people-vision-impairments-use-emoji\">http://www.perkinselearning.org/technology/blog/how-do-people-vision-impairments-use-emoji</a></p>\n<p><a href=\"http://www.perkinselearning.org/technology/blog/emojis-activities-digital-language-emotion\">http://www.perkinselearning.org/technology/blog/emojis-activities-digital-language-emotion</a></p>\n","tags":[{"name":"Accessibility","slug":"accessibility"},{"name":"Culture","slug":"culture"},{"name":"Design","slug":"design"},{"name":"Speech","slug":"speech"},{"name":"UX","slug":"ux"}],"video_presentation_url":"https://vimeo.com/336817812","video_documentation_url":"http://emojispeak.net","project_url":"http://emojispeak.herokuapp.com","headshot":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Headshot-768x432-1.png","title":"Headshot-768x432","alt":"krizia headshot","caption":""},"thumbnail_image":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thumbnail-1-1024x576.png","title":"Logo Thumbnail","alt":"Draft Logo of EmojiSpeak","caption":"Hand-drawn emoji with words Have You Heard Emoji Speak?"},"slide_show":[{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thesis-Slideshow-2-Copy-1024x576.png","title":"Emoji Dictionary","alt":"Homepage of EmojiSpeak","caption":"EmojiSpeak dictionary enables the Accessibility co-create emoji descriptions and usage context aka Urban Dictionary of emojis"},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thesis-Slideshow-2-1024x576.png","title":"Emoji Composer","alt":"Text to Speech Message Board","caption":"This platform plays the description of the emoji to expose possible inaccessible description"},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thesis-Slideshow-5-1-1024x576.png","title":"Case Study - Blind Participant","alt":"Laptop screen showing 3 apps","caption":"Composer is inspired by collapsing all 3 of the apps a blind user uses to send a tweet with Emoji"},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thesis-Slideshow-1-1024x576.png","title":"Sketches","alt":"3 sketches that contain a chatbot","caption":"Initial Designs leverage existing solutions for predicting search, and other Voice UI options such as a chatbot to define emoji"},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Thesis-Slideshow-1-Copy-1024x576.png","title":"Inclusive Design","alt":"Photo of mobile with emoji keyboard and Persona Spectrum addressing emoji keyboard problem","caption":"For future considerations, the designer attempts to expand the use case to redesign the emoji keyboard"}]}