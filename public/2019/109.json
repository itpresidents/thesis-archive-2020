{"student_id":"109","student_name":"Nicolas Escarpentier","student_slug":"nicolas-escarpentier","advisor_name":"Kathleen M Sullivan","title":"Inner Cadence","thesis_statement":"Inner Cadence is an interactive video wall that captures and amplifies people’s characteristic movements. By combining machine learning with motion capture, it reveals the audience’s individual identities by enhancing motion as a form of expression.","abstract":"Over the past year I have been exploring the idea of movement as a form of expression that’s inherently linked to our identity. It conveys individuality and emotion: we can be identified by our gait, or infer someone's mood by their pose. I want to maximize this expressiveness potential and explore ways of enhancing people's movements. I want to foreground motion by separating it from the body, silhouette, features, or clothes, making people rethink how they build their identity and their relation to others.\r\n<br><br>\r\nFor this purpose I created a 3-part video wall that analyzes people’s characteristic movements, classifies and amplifies them, enhancing its expressiveness. I used motion capture hardware and machine learning algorithms to showcase expanded forms of motion which feature different textures, forms and colors. Each wall is a single-user experience, and by placing the three walls next to each other, the audience will hopefully reflect and make connections between the repeating patterns that connect them together. \r\n","context_research":"I started my research by looking at choreography and what has been done around it. This pushed me to study diverse performance works from Merce Cunningham’s algorithmic choreography and Lucinda Childs’ “Dance” to Kate Sicchio’s “Hacking Choreography”. I also looked at movement depictions, such as “Ballet Rotoscoping” and “Performance in Zero Gravity”, and movement amplifications like “Forms” by Memo Atken and Quayola. While very useful for inspiration and overall context, these works go in the exact opposite direction I wanted to go: they generate choreography from certain parameters, where I want to generate parameters by analyzing movement. More on the technical side, I studied the “Choreographic Language Agent” by Wayne McGregor and the Open Ended Group, “Reactor for Awareness in Motion” by YCAM, and “Synchronous Objects” by William Forsythe. These descriptions of movements, while interesting, foreground the body, skeleton or silhouette, making the movement an action performed by a body, not an act of interest by itself. And it is here where I want to challenge the representations and how we think about motion on itself.\r\n<br><br>\r\nI also met with experts on the field of movement, dance and their intersection with technology: Mimi Yin, Elizabeth Coker and Kat Sullivan. From them I learned about different ways of tracking bodies and what matters in each case, as well as their experiences and conclusion. They also gave me their own outlooks on my ideas, helped me expand on how to accurately amplify the movements, and pointed me to more resources. This increased the list of books, articles and videos I researched on movement, dance, improvisation and identity.\r\n<br><br>\r\nMy own experiments started with a motion reactive Processing sketch, which pixelated and stylized the camera input only where there is motion. While interacting, I recorded people’s actions and skeletal data with a GoPro and Kinect camera respectively. This gave me a starting point on studying how people’s movements vary and what they do when faced with an interactive motion-based sketch. Later data captures, and the selection of joints to analyze for each movement type and to capture for the live amplification were all derived from these findings. \r\n","technical_details":"This installation captures the user’s skeletal data with a Kinect camera. This is sent to a Python server which normalizes, selects and classifies different types of movement according to a LSTM RNN algorithm. These labels are sent back to create and project the amplified graphics in openFrameworks and Unreal Engine. \r\n<br><br>\r\nFor the training data, I created a Processing sketch to visualize the clustered data form a KNN algorithm. From here, I chose the best combination of number of clusters, batch size and joints to analyze to create the labels to feed into the ML model.","further_reading":"","tags":[{"name":"Art","slug":"art"},{"name":"Identity","slug":"identity"},{"name":"Installation","slug":"installation"},{"name":"Machine Learning","slug":"machine-learning"},{"name":"Visual","slug":"visual"}],"video_presentation_url":"https://vimeo.com/336826936","video_documentation_url":"https://vimeo.com/331508586","project_url":"http://nicolaspe.com/portfolio/inner_cadence","headshot":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/npe_pixlr.jpg","title":"npe_pixlr","alt":"Nicolás Escarpentier","caption":""},"thumbnail_image":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/IC_archive_logo.jpg","title":"IC_archive_logo","alt":"Inner Cadence logo","caption":""},"slide_show":[{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/ic_screen-1024x576.jpg","title":"ic_screen","alt":"Inner Cadence screen prompt \"come and become movement\" with particles on the background","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/ic_UnfReal02-1024x576.jpg","title":"ic_UnfReal02","alt":"User moving with their amplified movement as particles","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/ic_prompt-1024x576.jpg","title":"ic_prompt","alt":"User standing in front of the screen with the prompt \"come and become movement\"","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/ic_UnfReal01-1024x576.jpg","title":"ic_UnfReal01","alt":"User moving with their amplified movement as particles","caption":""}]}