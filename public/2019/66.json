{"student_id":"66","student_name":"Jim Schmitz","student_slug":"jim-schmitz","advisor_name":"Kathleen M Sullivan","title":"The Modern World, Painted","thesis_statement":"I designed and implemented a new approach for applying style transfers to 360 pictures and video. Using contemporary and historical paintings and images from Google Street View, I created art that encourages viewers to appreciate what is beautiful or poignant about the modern world, right in front of us, and to look at it with new eyes.","abstract":"Applying a style transfer algorithm to 360 imagery allows us to create immersive experiences that provide the feeling of being inside a painted world.<br><br>\r\n\r\nA style transfer is a computational technique involving a neural network that can re-imagine a photograph in the artistic style of a selected painting. A 360 image is a panoramic (spherical) image that provides the viewer with the opportunity to look in every possible direction instead of the single direction provided by traditional fixed-view images.<br><br>\r\n\r\nThe standard style transfer algorithm can be applied to 360 images, but the result has undesirable artifacts because the algorithm doesn’t work well with the distortions of the equirectangular projection used to store 360 image data. For my thesis, I developed a new approach for applying style transfers that eliminates these artifacts and properly transfers a painting’s style to a 360 image in an even and continuous fashion.\r\n<br><br>\r\nGoogle Street View is an expansive resource of 360 imagery capturing the modern world. In aggregate the images are a reflection of every way the world could possibly be described: beautiful, distressing, amazing, opulent, disorderly, tragic. This richness and complexity is often overlooked by people using the service for mere navigation purposes. By applying my style transfer algorithm to these images, I can draw attention to what is remarkable or meaningful about these locations and encourage viewers to look at them with new eyes and to consider the actual physical locations they depict with a new perspective.","context_research":"The biggest influence on my thesis project is the research paper “Artistic Style Transfer for Videos and Spherical Images” by Ruder, Dosovitskiy, and Brox (2018). The paper presents a technique for applying a style transfer algorithm to 360 imagery by dividing up the image into six cube faces. I greatly appreciated this paper and was inspired to think of ways the algorithm could be further improved.\r\n<br><br>\r\nArtistically I am inspired by the work of video artist Nam June Paik (1932-2006). Although his artistic medium is different from mine, his willingness to explore and modify the underlying electronics of television and video equipment motivates me and my work with machine learning and computational tools. I am also inspired by artist and neuroscientist Santiago Ramón y Cajal (1852-1934). His detailed drawings of neurons pioneered the field of neuroscience and earned him the Nobel Prize in 1906. It is because of his skills as an artist that he distinguished himself as a scientist and it is because of his work in neuroscience that he made a unique contribution to the art world. My ambition is that my background in technology and quantitative research and my new knowledge of the art world will empower me to have a similar impact.","technical_details":"The foundation of my style transfer algorithm is a “TensorSphere” data structure I conceived and developed for this project. A TensorSphere organizes data (3 color channels) in a spherical structure instead of the typical N-dimensional grid of a generic Tensor. Tensorspheres support indexing using spherical coordinates while also minimizing the number of points used to circumscribe the sphere, facilitating data access in a way that can eliminate the unwanted artifacts observed when applying a style transfer algorithm to 360 imagery.\r\n<br><br>\r\nThe TensorSphere design and my style transfer algorithm will be formally documented and submitted for publication at a later date.","further_reading":"<p>To understand what is interesting about my project, let's first review what a style transfer is and how it works.</p>\n<p>A style transfer is a computational technique involving a neural network that can re-imagine a photograph in the artistic style of a painting. Put more simply, if you give it a photograph and a painting, it will output something that has the same content as the photograph but with the style of the painting.</p>\n<p>Compare the row of houses photographed in the upper left corner with the other three images. You see that the three images still look like the same row of houses but they seem to duplicate the artistic style of famous paintings by Turner, Van Gogh, and Munch.</p>\n<p>These examples come from the seminal paper on the subject titled <a href=\"https://arxiv.org/abs/1508.06576\">A Neural Algorithm of Artistic Style by Gatys, Ecker, and Bethge</a>, published in 2015. Since that time many more research papers have been published that iterate on the idea of using a neural network to create images that mimic a painting's artistic style. I've always been fascinated with style transfers and my motivation to explore the topic contributed to my decision to attend ITP.</p>\n<p>During last year's Project Development Studio class I used the opportunity to obtain images from the Google Street View API and applied the style transfer algorithm to make what looks like animated paintings. I wanted to create 360 style transfers for that class but it was unachievable for me at the time.</p>\n<p><iframe src=\"https://www.youtube.com/embed/HLAjwoQZFtU\" width=\"560\" height=\"315\"></iframe></p>\n<p>Last semester I was asked to create illustrations for an <a href=\"https://itp.nyu.edu/adjacent/issue-4/binary-borders/\">Adjacent article</a> using style transfers. For this project, I developed a <a href=\"https://ixora.io/itp/project_development_studio/adjacent-illustrations/\">new approach I call cylindrical style transfers</a>. This is just like a regular style transfer except that the right and left sides of the image join together seamlessly.</p>\n<p><img src=\"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/taiwan3.png\" /></p>\n<p>This illustrates an important challenge with style transfers. The results of the style transfer algorithm are somewhat random; it also has no way of knowing that the right and left sides of the image are supposed to join together. Therefore, in the above image, there is no way to ensure that the styling of the truck on the left side of the picture will match the styling of the truck on the right side of the picture. Any differences, even small ones, will cause a noticeable and unpleasant seam.</p>\n<p>360 imagery uses something called an equirectangular projection to represent the image data. Ideally a 360 image would be represented with some kind of spherical arrangement of pixels, but computers (and the neural network the style transfer algorithm is based on) can only work with rectangular grids of pixels. An equirectangular projection is an appropriate pixel arrangement that is also computationally easy to work with.</p>\n<p>The most straightforward example of what an <a href=\"https://en.wikipedia.org/wiki/Equirectangular_projection\">equirectangular projection</a> is that surely you've seen before is a world map:</p>\n<p><img src=\"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/world.png\" /></p>\n<p>By <a href=\"http://commons.wikimedia.org/wiki/User:Strebe\">Strebe</a> - Own work, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=16115228\">Link</a></p>\n<p>The challenge posed by cylindrical images is amplified with 360 images. Not only do the right and left sides of the image need to join together seamlessly, but the entire top and bottom edges need to each join together in a single point similar to the way locations on the map above join together at the north and south poles. In addition, the pixels near the top and bottom of the image are stretched out so the style transfer algorithm needs to be applied in a way that matches that distortion.</p>\n<p>One approach for applying style transfers to 360 imagery is to ignore these challenges and let the algorithm do the best it can with the image as it is. This can work but the result often has unpleasant artifacts; consider the below animation highlighting the problems. In addition to the unsightly seam, near the zenith and nadir it looks like the style is being pulled into a black hole.</p>\n<p><iframe src=\"https://www.youtube.com/embed/m2D0j4zHXt0\" width=\"560\" height=\"315\"></iframe></p>\n<p>In 2018, Ruder, Dosovitskiy, and Brox published the paper <a href=\"https://arxiv.org/abs/1708.04538\">Artistic style transfer for videos and spherical images</a>. The researchers addressed the challenges of style transfers applied to 360 imagery by employing a cubic projection to remap the equirectangular projection to a set of six cube faces. The style transfer algorithm is applied to each face of the cube separately. The six styled cube faces are then joined together in a way that blends the seams and creates a new equirectangular projection. This approach eliminates the distortion problems at the nadir and zenith and reduces the appearance of seams. Although this algorithm achieves improved results, it still has visible artifacts caused by the cubic projection. If you watch their <a href=\"https://www.youtube.com/watch?v=pkgMUfNeUCQ\">demo video posted to YouTube</a> you will notice the orientation of the cube and the location of the cube's corners. Below is a screenshot of one of those corners:</p>\n<p><img src=\"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/cube_corner.png\" /></p>\n<p>When I read this research paper I was intrigued; I had recently added efficient <a href=\"https://ixora.io/projects/camera-3D/monoscopic-360-video/\">360 rendering functionality</a> to my Processing library <a href=\"https://ixora.io/projects/camera-3D/\">Camera3D</a>. I immediately started thinking about alternative approaches. These ponderings stuck with me and grew. My thesis project is the direct result of my attempt to think of a better algorithm for 360 style transfers.</p>\n<p>My 360 style transfer algorithm applies the style transfer algorithm developed by Gatys, Ecker, and Bethge to 360 imagery in a way that respects the distortion of the equirectangular projection and (when used properly) is completely seamless. Look at the below animation made with my algorithm and compare with the similar one shown previously:</p>\n<p><iframe src=\"https://www.youtube.com/embed/f-Ya9AyMvu0\" width=\"560\" height=\"315\"></iframe></p>\n<p>Central to my approach is a new spherical data structure I will call a TensorSphere. This data structure can access pixel data using spherical coordinates, allowing me to apply the style transfer algorithm to the entire image at the same time in a way that addresses all of the seam and distortion challenges.</p>\n<p>An animation showing the complete 360-degree views is below. The source image is from <a href=\"https://www.google.com/maps/@48.870265,2.332304,20z\">Paris, France</a> and was obtained with the <a href=\"https://developers.google.com/maps/documentation/streetview/intro\">Google Street View API</a>. The style image is <a href=\"https://www.artic.edu/artworks/81551/the-place-du-havre-paris\">The Place du Havre, Paris (1893) by French Impressionist artist Camille Pissarro (1830-1903)</a> from <a href=\"https://www.artic.edu/\">Art Institute Chicago</a>.</p>\n<p><iframe src=\"https://www.youtube.com/embed/-2bnrUFbmRs\" width=\"560\" height=\"315\"></iframe></p>\n","tags":[{"name":"Art","slug":"art"},{"name":"Machine Learning","slug":"machine-learning"},{"name":"Python","slug":"python"},{"name":"VR/AR/Immersive","slug":"vr-ar-immersive"}],"video_presentation_url":"https://vimeo.com/336826351","video_documentation_url":"","project_url":"","headshot":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/default.png","title":"default","alt":"default","caption":""},"thumbnail_image":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/260.jpg","title":"260","alt":"360 logo","caption":""},"slide_show":[{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/260.jpg","title":"260","alt":"360 logo","caption":""}]}