{"student_id":"94","student_name":"Michael J. Blum","student_slug":"michael-j.-blum","advisor_name":"Stefani Bardin","title":"Paralang","thesis_statement":"Through an educational learning hub and an interactive playground, both hosted on the same web app, Paralang aims to cultivate literacy, inspire curiosity, and arouse concern with respect to emerging neural language models.","abstract":"Recently released, state of the art language models have been shown to be able to produce text that is nearly indistinguishable from that produced by humans. \r\n<br><br>\r\nThese recent advances, which have proved plenty controversial within machine learning circles, have caused ripples in the general media landscape as well, where coverage has been largely hyperbolic, excessive, and occasionally uninformed or even incorrect. \r\n<br><br>\r\nWith the belief that this natural language generation technology, more than mere novelty, will gradually assume a more and more pervasive role in our everyday lives, I wanted to intervene, however modestly, and provide an accessible, beginner-friendly platform to help secularize this technology and elaborate on some of its inner-workings as well as its repercussions both for us as individuals and a society. I’d like to help answer questions like: what makes these recent advances so compelling and new? Or: how might existing societal problems by reproduced and reinforced by these advanced language models?\r\n<br><br>\r\nUltimately, my aim is to help cultivate a more level-headed literacy as well as inspire both a sense of informed curiosity and concern with respect to these emerging models and their ramifications, with an emphasis on the recent and state of the art (particularly Google’s BERT and OpenAI’s GPT-2).\r\n<br><br>\r\nThe platform consists of two components, both hosted on a single web app. One is educational, revolving around a learning hub, glossary, and resources curated for all skill levels — newcomer, intermediate, and advanced. The other is interactive, comprising of a “playground” encouraging hands-on experimentation with some of the language models featured in the educational component. \r\n<br><br>\r\nAltogether, the platform is built to accommodate non-linear engagements — users can begin with the learning hub and progress through to the playground, or simply jump to the playground, or maybe even just skip around between glossary and resources.","context_research":"The ground and basis of my research consisted in familiarizing myself with the field of natural language processing, from different strategies of word and sentence embedding to classification, dependency parsing, understanding, and generation, as well as more advanced approaches involving recurrent neural networks and sequence to sequence models. From there, my primary focus was on state of the art language models, particularly those powered by the Transformer neural network architecture. Towards this end, Stanford’s CS224n course, Jay Alammar’s illustrated blog posts, as well as Google and OpenAI’s research papers on arxiv.org were particularly illuminating.\r\n<br><br>\r\nSome of the undergirding conceptual motivations of the project were spurred on by various sources, from academic and non-academic articles on bias baked into datasets used in machine learning pipelines to Benjamin Bratton’s writings, in which he delineates a compelling, original conception of the relationship between human being, human creation and artifice, and the machinic reflection of what “human” has been, is, and can be. \r\n<br><br>\r\nWhile some of the practical and technical motivations of the project were inspired by similar educational deployments of neural network models such as Joel Simon’s Ganbreeder and a project jointly helmed by Google Brain and Georgia Tech called GANLab.","technical_details":"The skeleton of the project is a web application written primarily in HTML and CSS and deployed using the Flask microframework for Python, in tandem with the Jinja2 templating language. The neural network models featured in the playground section of the web app are all either vanille, fine-tuned, or fine-tunable versions of models originally authored by Google or OpenAI (more specific details and attribution can be found on the sections of the web app on which they appear). These models are also deployed using Flask, Python, and Flask-WTF for handling forms and user input.","further_reading":"<p>Algorithmic innovation and the dramatic increase in computing power have allowed machine learning researchers to train models on ever larger data sets, yielding ever more impressive and convincing results. This is essentially the story of modern deep learning, which many argue began in earnest in 2012, with revolutionary advances in image recognition and classification tasks thanks to deep convolutional neural networks trained with the help of graphics processing units (GPUs).</p>\n<p>Over the course of this past year, a similarly revolutionary step forward not unlike what the domain of image recognition underwent in 2012 can be observed in the arena of natural language processing, with major repercussions for the classification, understanding, and generation of text.</p>\n<p>The mid-February release of OpenAI’s GPT-2 language model only further consolidated this phenomenon, and did so with a bang, drawing controversy and attention for its capacity to generate text nearly indistinguishable from what one would expect from a real person. Particularly in the wake of OpenAI’s decision not to release the full, large-sized model of GPT-2, the discourse surrounding their research swirled with swooning hyperbole and dangerous exaggeration, while a vocal bunch in the machine learning community felt that OpenAI itself wasn’t totally blameless of some amount of aggrandizing and fear-mongering as well. But even while it could certainly be argued that GPT-2’s accomplishments are more attributable to increase in sheer data and processing power rather than groundbreaking algorithmic or architectural advances, the model’s results in terms of natural language generation are still the most eerily convincing of any we’ve seen yet.</p>\n<p>As fast.ai co-founder Jeremy Howard remarked in the aftermath of GPT-2’s release, we have the technology to “totally fill the internet with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter.”</p>\n<p>It is precisely this context which motivates my project, and to which I intend my project to respond.</p>\n<p>My earlier ideation and plans for project had a very different shape and texture than what it eventually took form as. Originally, its goals were twofold:</p>\n<ul>\n<li>To bottle the surprise and wonder I first felt when generating text with the aid of the aforementioned algorithms or tangentially related ones, trained on my own data to successful effect, and share this experience with beginners and laypeople.</li>\n<li>To reflect on the encounter with the algorithmic reflection of ourselves — allowing users to input their own data into an advanced neural language model, and derive from this model’s output something like an uncanny heuristic for understanding themselves in an even slightly different way. This admittedly ambitious, arguably confusing goal was of a piece with Benjamin Bratton’s contention that “the real philosophical lessons of AI will have less to do with humans teaching machines how to think than with machines teaching humans a fuller and truer range of what thinking can be (and for that matter, what being human can be).”</li>\n</ul>\n<p>Upon reflection, though, I felt that foisting such advanced language models, in tandem with so lofty a philosophical scaffolding, upon laypeople and other newcomers to machine learning would ultimately not work out as I had hoped. Sure, it’d provoke some surprise, but would likely fizzle and terminate as little more than an ephemeral novelty, and wouldn’t necessarily contribute toward developing a more informed and intuitive feel for the technology involved. And failing to do so, for whatever reason, just felt a little irresponsible to me.</p>\n<p>After GPT-2, I felt that this educational, clarifying motive was all the more urgent, and deserved to be pushed to the fore.</p>\n<p>I also felt like a resource or platform for learning about this sort of thing, tailored to a certain level of non-expertise, was simply missing. Even most beginner’s explainers are constructed as an on-ramp for aspiring researchers and engineers, and invariably include frequent reference to phrases like “stochastic gradient descent” or “softmax activation function,” which, however integral for people preoccupied with building and working with these models, only serve as intimidating, knotty impediments to laypeople who are simply curious to learn about the subject.</p>\n<p>Negotiating the dynamic between making the material as welcoming and compelling as possible without, crucially, resembling a crude or even incorrect caricature of what I’m attempting to simplify, is surely a formidable task. But, ultimately, it’s one that is oddly consonant with so much of what, at bottom, much work in machine learning is about. And that is <i>finding the right degree of generalization</i> — laboring and landing in that sweet spot where you’re neither exactly reproducing the data at hand, nor haphazardly running off into error or nonsense.</p>\n","tags":[{"name":"Data","slug":"data"},{"name":"Education","slug":"education"},{"name":"Machine Learning","slug":"machine-learning"}],"video_presentation_url":"https://vimeo.com/336818022","video_documentation_url":"","project_url":"","headshot":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/michealblum.png","title":"michealblum","alt":"micheal blum portrait","caption":""},"thumbnail_image":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Subtract.jpg","title":"Subtract","alt":"Logo","caption":""},"slide_show":[{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/Recent-SOTA-1024x640.jpg","title":"Recent SOTA","alt":"Learning hub screenshot","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/learning-hub-test-1024x640.jpg","title":"learning-hub-test","alt":"Leaning hub screenshot","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/syllabus-1024x640.jpg","title":"syllabus","alt":"Syllabus screenshot","caption":""}]}