{"student_id":"37","student_name":"Dan Oved","student_slug":"dan-oved","advisor_name":"Gabriel Barcia-Colombo","title":"The Self-Driving Human","thesis_statement":"The self-driving human is a device and performance where an intelligent portable agent makes decisions for a person in the real world.","abstract":"As technology increasingly becomes integrated with our daily lives, we rely on it more and more to make decisions for us, from things such as how we should get from one place to the next (Waze), to what we should have for dinner (yelp, tripadvisor), to what content we should see on the internet (social media), to who we should date (dating apps).    What would a future feel like when all of our decisions are made by these machines?  \r\n<br><br>\r\nThe self-driving human simulates this scenario by making choices for the human in areas that are currently not decided by technology.  It’s a portable device that detects objects around the person using a camera and machine learning, and gives commands to the user on how to interact with the environment based on an arbitrary algorithm that changes day by day.  It allows the person to outsource thinking and decision making to an algorithm that they do not entirely understand.\r\n<br><br>\r\nThe agent’s decision space is limited to what the machine has been trained to see.   The user does not know what the logic of the algorithms are, and while the machine does know the objective, it does not know if it is good or bad.   What happens when the objectives of this machine differ from the objectives of the person its meant to serve?   How do the choices made by the algorithm collide with the humans desires when they’re disconnected from the biases and emotions of that the person has learned throughout his or her life?\r\n<br><br>\r\nThe device is portable and works totally offline using machine learning on the edge, allowing for real-time response even where there is no internet connection, and maintaining privacy as all data stays on the device.\r\n<br><br>\r\nFor the performance it is carried by me in the real world, where I listen to its commands and attempt to do as we’re instructed, no matter how uncomfortable it makes me.\r\n","context_research":"This thesis is heavily influenced by previous performance art pieces where someone was given instructions by a piece of technology or a human to perform.  These include: Nicole He’s The Best Art, where a machine generated art projects for her to make, Anastasis’ The Sybil Society, where identities and conversations were created and performed by people, Lauren McCarthy’s Social Turkers, where she would go on daily online dates and be instructed by human Mechanical Turk workers on what to do and say, and Max Hawkin's Randomized Living, where he lets randomized computer programs decide where he lives over two years.  It’s also influenced by projects that explore the subjectiveness and black-box nature of machine learning algorithms, such as the Normalizing Machine by Mushon Zer-Aviv , Dan Stavy and Eran Weissenstern, in which participants attempt to identify who looks normal from previous participants and a machine analyzes these decisions and adds them to its aggregated algorithmic image of normalcy, and So Kanno and Yang02 ‘s Asemic Languages, in which a robot invents and draws new characters that look as if they are important, but in reality are purely aesthetic, showing that a robot only knows what it has been taught.  \r\n<br><br>\r\nThere have been lots of writing around these subjects that I’ve read.  About decision making and choice, Thinking, Fast and Slow by Daniel Kahneman explores the two systems of the mind and their affects on our decision making and choices - this book revealed to me the massive amounts of decisions we make and are not aware of, and how they are influenced in different scenarios.  Algorithms to Live By by Tom Griffiths and Brian Christian explore how algorithms used by computers, can be applied to human decision making.  Essays in 25 Way of Looking at AI were hugely influential, including the Purpose Put into the Machine by Stuart Russel, which discusses purposes that humans put into machines and the failure when the machine’s objectives differ from our own.  Homo Deus by Yuval Harrari was the largest influence for me to dive into this thesis, in particular this quote:  \r\n<br><br>\r\n“Every day millions of people decide to grant their smartphone a bit more control over their lives…  In pursuit of health, happiness and power, humans will gradually change first one of their features and then another, and another, until they will \r\nno longer be human.”\r\n","technical_details":"The agent is a portable device consisting of the Coral Edge TPU Accelerator and a  Raspberry Pi 3b+.  It uses a camera and machine-learning to detect objects in front of the person, and gives commands to the user on how to interact with those objects.  It also uses environment sensors that detect things such as temperature, motion, and humidity, and uses these to arbitrary control what the algorithm logic is.  ","further_reading":"<p>At any given moment, the machine observers objects in the environment around the person, and will choose an action related to one of those objects for the human to perform.  These actions pertaining to an object are scraped from various how to articles and books, each with different opinions on what’s the right way to do something, such as what diet to have, how to optimize some aspect of living.  Depending on the day of week and temperature, actions from specific opinionated texts will be selected.</p>\n","tags":[{"name":"Critical Theory Art","slug":"critical-theory-art"},{"name":"Experiment","slug":"experiment"},{"name":"Machine Learning","slug":"machine-learning"},{"name":"Performance","slug":"performance"},{"name":"Speculation","slug":"speculation"}],"video_presentation_url":"https://vimeo.com/337308779","video_documentation_url":"https://vimeo.com/331468540/","project_url":"https://www.danioved.com/self-driving-human","headshot":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/thesis-12-169x300-1.jpg","title":"thesis-12-169x300","alt":"dan headshot","caption":""},"thumbnail_image":{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/MainImageForSubmission-1024x682.png","title":"Stranger Walk Prompts","alt":"Stranger Walk Prompts","caption":""},"slide_show":[{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/thesis-8-1024x683.jpg","title":"thesis-8","alt":"portable prototype, with the coral and raspberry pi and camera. held in a hand.","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/PetThatDog-1024x576.jpg","title":"Pet That Dog","alt":"Pet that Dog","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/SelfDrivingHuman-1024x576.jpg","title":"say hello to that person","alt":"say hello to that person","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/thesis-7-1024x683.jpg","title":"thesis-7","alt":"camera on the back","caption":""},{"src":"https://itp.nyu.edu/thesis2019/wp-content/uploads/2019/04/fanny-1-1024x768.jpg","title":"fanny-1","alt":"the device inserted into a fanny pack, with a spy camera sticking out.","caption":""}]}