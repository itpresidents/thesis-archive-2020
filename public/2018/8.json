{"post_id":403,"student_id":"8","student_name":"Hayeon Kim","student_slug":"hayeon-kim","advisor_id":"114","advisor_name":"Adaora Udoji","advisor_slug":"aeu4","project_title":"Droplet: an AR Experience Design System","project_question":"The goal of <i>Droplet</i> is to deepen our understanding of AR interface, virtual space, and application design. I designed and built a tool that allows users to manipulate AR in expanded ways. \r\n<br />\r\n<br />\r\nI started with the question how could I reduce the feeling of disconnectivity between virtual and physical world? To smooth the meeting of physical and virtual worlds, I designed several AR interactions that go beyond the mobile screen and into the environment. My goal was to provide a new experience for users to change the physical environment. \r\n<br />\r\n<br />\r\nA concept from <i>Droplet</i> demonstrates how augmentation can enhance a user’s engagement by employing an AR to control real object. My objective of this project is to mediate augmentation and reality by diminishing the barrier between the virtual and physical world.\r\n<br />\r\n<br />\r\nI build a set of prototyping interfaces where augmentation takes an essential role as the controller. It \"a visual controller\" works; when a droplet is given to start in a virtual space, the physical object built with neo pixels will be operated by a user's dropping motion. It is the interactable element is calling assigned states to change the physical object so that it looks gradually soaking from the virtual waterdrop. Since the idea of getting users to control intuitively is my goal, user experiences I want to create though augmentation is prominent. Next, thunder has interaction with the physical object as well. In particular, thunder is created in a virtual space shown on your device. Once a user hits it the physical object will be x-rayed to reflect the thunders at a certain level. Lastly, a pinwheel is rotating at different speed consoled by users' interactivity, it will create augmentation of flying particles.\r\n<br />\r\n<br />\r\nIt overlays augmented environment with a physical environment to interact while using this app. It enables virtual objects to be controlled at precise positions and depths. As a virtual object composed of GUI appears in mid-air shown on their mobile screen, the user can change the physical object. Users thereby enjoy the interaction with the physical environment using a virtual object.","short_description":"<i>Droplet</i> is an experimental system for designing intuitive, natural interfaces for AR applications. It consists of a series of prototypes aimed at demonstrating how a fluid connection can be achieved between a user, physical environment, and virtual space, through tactile, visual-based on-screen manipulation of networked objects.","further_reading":"<p>During my thesis study, I break down this idea that blurs boundaries between virtual and physical environment and made series focusing on interfaces. I make a set of design to consider AR interface, so it would encourage people to engage in interactions.'Droplet' is putting me to solve interaction design problems and finding what user-centered design works the best in AR context.   </p>\n<p>Generally speaking, it is easy to find that AR applications available on market are driven by what kind of technology is available for an AR-enhanced project, rather than being driven by the type of human experiences.  To resolve this, I bring user experience more prominently into the AR context while using augmentation. First, it is essential that keeping augmentation simple prevents from distraction. If AR elements are too visually complex, most people won’t be as effective at using intensive or distracting AR, especially if the augmentation involves instruction to use. With this in mind, it’s imperative that AR experiences avoid overstimulating users. Since the more an augmentation attempts to grab attention, the more user will focus on it, I intend to transmit essential information to the user, and in so doing, helps ensure their understanding of how to use. </p>\n<p>While focusing on user-centered interaction, the innovative design solutions I can come up with is to truly enhance reality.  In order to encourage interaction with the environment, graphics and dynamic animation can be inserted as not only as a guide but a part of the experience itself. It doesn’t always have to be text or instructions.</p>\n<p>To research about this, I got inspired by products such as Roomba, Line-us, Sphero. What if I can control them with a new and easy interface, rather than a tech-oriented interface? This would be entertaining and user-friendly.  It will move when you drag it and it will light on/off when a user interacts with it on a mobile screen. it will let them engage in a new level of experience.  </p>\n<p>First, I attempt to make an object with lots of pixels built inside of the object and wifi Arduino included. When a user opens the app and locates in front of the object, it will play with the virtual object the physical object is reacted to it. I played with ARKit and Vuforia and physical elements using wifi module to connect.  As interaction design goes, droplet and thunder are the 3D volumetric model that rendered in a context of AR.  Additionally, as I look forward simple and minimal visual cues, I believe it works as well. Users can generate a raindrop and with this digital creature, it will allow them to manipulate the object.  Since the context will drive the possible interactions that would lead users AR experience, I chose droplet because it drops down, so users easily try to swipe down to interact with it.  When raindrop is created in a virtual space and it is the interactable element itself is calling states to change the physical object. In particular, the built-in LEDs are reacting to it.  It will eventually look like gradually soaking from the virtual rain.</p>\n<p>Second, thunder is created in a virtual space shown on your device. and when it hits the object, the object will be x-rayed to reflect scene of thunders.  Lastly, a pinwheel is operated at different speed consoled by users' interactivity, it will create virtual particles.</p>\n<p>From the series of works I’ve been working on, I conclude with a summary of virtual action freeing physical stillness have sorted out.  Although the augmented reality interfaces used in this study were basic, more advanced interfaces can be applied to this so that it would improve the variety of properties. I also believe applying this system to other uses such as in AR play interface and will contribute to the augmented reality of perceived user interfaces.</p>\n<p>Related works<br />\nexTouch and Reality Editor at MIT media lab are existing approaches which well-suited to my study and model information systems.</p>\n<p># MR\\AR  #UX\\UI  # Installation #tangible</p>\n<p>Reference<br />\nEvans, K., Koepfler, J. (2017). The UX of AR: Toward a Human-Centered Definition of Augmented Reality. User Experience Magazine, 17(5).<br />\nMARIO: Mid-air Augmented Reality Interaction with Objects<br />\nManovich, Lev ’The poetics of augmented space: learning from Prada’<br />\nBuxton, Bill ‘Less is More’</p>\n","portfolio_icon":{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/thumbnail-4.png","title":"droplet","alt":"title","caption":""},"topics":[{"name":"UX\\UI","slug":"uxui"},{"name":"VR\\AR","slug":"vr-ar"}],"video_presentation_url":"https://player.vimeo.com/video/270188837","video_documentation_url":"https://vimeo.com/itsmonakim/droplet","project_url":"https://www.hayeonkim.net/droplet","description":"","featured_image":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/thumbnail-4.png","title":"droplet","alt":"title","caption":""}],"slide_show":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/mona_final.gif","title":"mona_final","alt":"demo","caption":""}]}