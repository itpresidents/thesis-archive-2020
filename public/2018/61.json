{"post_id":402,"student_id":"61","student_name":"Ellen Kim","student_slug":"ellen-kim","advisor_id":"124","advisor_name":"Kathleen Wilson","advisor_slug":"ksw222","project_title":"At Home","project_question":"Many people today have smart home devices. Some even interact with their devices more than they interact with other people. My project addresses the question, \"What would it be like if these devices could provide more than information such as the weather, shopping assistance, and entertainment on-demand? What if it were possible to create a device that could collect voice statements that a user makes to smart devices, observe the user's daily patterns, and analyze their emotional state?” \r\n<br />\r\n<br />\r\nIf devices could analyze the way users interact with them and users' emotional states, users could monitor their emotional health and acquire deeper knowledge of themselves. Allowing users to gain a better understanding of how they are feeling could help them make better decisions. \r\n<br />\r\n<br />\r\n\"At Home\" is a sentiment-journaling device that helps monitor the emotions expressed in a user's interactions with smart home devices. It is valuable to monitor these interactions because people's interactions with devices are sometimes more revealing than their interactions with other people. Through data visualization, users will be able to track the range of emotions they exhibit during the day while interacting with their home monitoring devices. Also, a larger goal for the future is for smart home devices to better assess your mood better and to have the ability to adjust their behaviors along with other IoT devices based on your mood.","short_description":"What if a smart home device could support people emotionally by providing feedback like friends and family do when they are around?","further_reading":"<p>Ever since designing an app at work for a home security camera in the summer of 2017, I have been fascinated by the concept of smart home devices, a popular key phrase in the age of the Internet of Things. </p>\n<p>When I consider Alexa or Google Home, I feel that the data from these devices can be used to identify my daily life and who I am. I found myself very interested in data visualization and human emotions and I feel that the experiments and the research I did during the semester really encouraged me to explore the visualization of my own data more. <i>At Home</i> is a stand-alone device that connects to a Pi camera and Voice HAT to detect your facial expressions and voice. It then analyzes the user's sentiments by using “Microsoft Azure emotion API” and “OpenVokaturi API”. The values obtained create visual shapes based on the analysis of emotions and are displayed on the screen connected. </p>\n<p>Initially, the target audience of <i>At Home</i> was myself. I planned to observe how this kind of visualization might help me to better understand my state of mind and take steps to take better care of myself. It is not always easy to know or admit to yourself how stressed you are, or how you are really feeling. By analyzing my facial expressions and tone of voice, I tried to explore the possibility that <i>At Home</i> would be able to detect and visualize the kinds of emotions I often fail to recognize, so that I could begin to acknowledge and deal with them more effectively.</p>\n<p>I started gathering my own interactions with the device and observed my daily activities. I recorded my data from March to April and analyzed my facial expressions and voice through the APIs. Through this research, I was able to gather eight emotions (anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise) from my facial expressions and five emotions (anger, fear, happiness, neutral, and sadness) from my voice. Then, I visualized these emotions in a graph. </p>\n<p>After using the device during several time intervals during the day, I was able to see the collection of my commands with an emotional analysis in a spider graph shape. Setting up the minimum bound of 0.0001 to maximum bound of 1.0, I created a spider graph and put the motions in order according to the timeline.<br />\nInterestingly, I was having somewhat of a variety of emotions throughout each day. For most days from the end of March until early April, around 12:40 PM, my emotional status was very high in “disgust” and “sadness” compared to other days. Also, compared to April 3, on March 31, which was a day on the weekend, I had a much higher positive emotional status. Although I was not aware of the fact that it was a weekend, I tended to talk more and asked about more content related to entertainment. However, when the graph showed negative indicators, I tended to ask more about the next day's schedule. Reflecting on these days, I now realize that I was unconsciously checking my whole schedule to confirm that I had nothing that needed to be done later. </p>\n<p>There are some limitations to the state of the device, as the voice API currently only detects five emotions. So it is possible that the graph shows the change in the existing values. It is always a challenge to analyze your emotional status. In the future, if I am able to get datasets from different emotion-analyzing sources, I may be able to obtain more accurate results. With using <i>At Home</i> I was able to monitor my daily emotional changes and discover myself.</p>\n","portfolio_icon":{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/thumbnail-13.jpg","title":"thumbnail","alt":"thumbnail","caption":""},"topics":[{"name":"Data","slug":"data"},{"name":"Tool\\Service","slug":"toolservice"}],"video_presentation_url":"https://player.vimeo.com/video/269232615","video_documentation_url":"","project_url":"https://kimellen.weebly.com/thesis","description":"","featured_image":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/thumbnail-13.jpg","title":"thumbnail","alt":"thumbnail","caption":""}],"slide_show":[{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/image1-1.jpg","title":"image1","alt":"image1","caption":"image1"},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/image2-1.jpg","title":"image2","alt":"image2","caption":"image2-single event"},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/image3.jpg","title":"image3","alt":"image3","caption":"image3-data"},{"src":"https://itp.nyu.edu/thesis2018/wp-content/uploads/2018/04/image4.jpg","title":"image4","alt":"image4","caption":"image4"}]}